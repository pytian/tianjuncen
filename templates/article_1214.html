<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title></title>
</head>
<body><!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="../static/css/bootstrap.min.css">

    <title>"怎样度量生成的语言"</title>
    <style>
        ul {
            list-style-type: none;
        }

        li {
            float: left;
            margin: 20px;
        }

        .row {
            border: rgba(0,0,0,0.3) 1px solid;
            margin: 10px;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <p>
                "怎样度量生成的语言"
            </p>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <p>
                生成文本是很多 NLP 任务的核心，但如何恰当地评估生成语言的「自然性」是很有难度的。好的评估指标应该不仅能捕捉到生成结果的质量，还能考虑到生成结果的多样性，这对对话或故事生成这类需要创造性的开放性任务尤为关键。

人类评估通常被视为金科玉律，既能捕捉质量也不会忽略多样性。但是，统计评估（即在参考测试集上的困惑度）也能捕捉多样性，因为它确保模型为新句子分配合理的概率，但是困惑度（Perplexity）无法提供充分的质量评估。

目前困惑度在语言模型等领域中是应用最广泛的度量方法，它刻画的是语言模型估计一句话出现的概率。困惑度有一个非常直观的理解，即我们可以认为它是平均分支系数。也就是说语言模型预测下一个词时平均可选的数量，即模型对下一个词的「困惑程度」。

现有的将统计评估和人类评估结合起来的方法是点对点的，这会导致有误导性的性能评估。常见的方法是通过概率模型的困惑度来评估多样性、通过对束搜索输出进行人类评估来衡量质量。这就会产生一种错觉：某个模型质量高且具备多样性，而现实则是这个模型要么只具备多样性，要么只能输出高质量的结果。
            </p>
        </div>
    </div>
</div>

<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="../static/js/bootstrap.min.js"></script>
</body>
</html>