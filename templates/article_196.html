<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title></title>
</head>
<body><!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="../static/css/bootstrap.min.css">

    <title>Google开源的BERT有什么特点？</title>
    <style>
        ul {
            list-style-type: none;
        }

        li {
            float: left;
            margin: 20px;
        }

        .row {
            border: rgba(0,0,0,0.3) 1px solid;
            margin: 10px;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <p>
                Google开源的BERT有什么特点？
            </p>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <p>
                <blockquote>BERT 开源实现尽管如前所述 BERT 的效果惊人，但<strong>预训练所需要的计算力同样惊人，一般的开发者基本就不要想着能复现了</strong>。BERT 的作者在 Reddit 上也表示预训练的计算量非常大，Jacob 说：「OpenAI 的 Transformer 有 12 层、768 个隐藏单元，他们使用 8 块 P100 在 8 亿词量的数据集上训练 40 个 Epoch 需要一个月，而 BERT-Large 模型有 24 层、2014 个隐藏单元，它们在有 33 亿词量的数据集上需要训练 40 个 Epoch，因此在 8 块 P100 上可能需要 1 年？16 Cloud TPU 已经是非常大的计算力了。」但是，谷歌团队开源了 BERT 的预训练模型，我们可以将它们用于不同的 NLP 任务。这节省了我们大量计算力，同时还能提升已有模型的效果，因此做 NLP 任务前，你可以先用预训练的 BERT 试试水？BERT 实现地址：https://github.com/google-research/bert其实目前已经有很多开发者将 BERT 预训练模型应用到它们自己的项目中，包括抽取句向量、句子相似性判断或情感分析等，下面两篇文章简单介绍了如何将 BERT 预训练模型迁移到你的项目中：

小数据福音！BERT 在极小数据下带来显著提升的开源实现

两行代码玩转 Google BERT 句向量词向量

<a href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9310443071119729141%22%7D&amp;n_type=0&amp;p_from=1"><img class="alignnone size-full" src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=3050602859,2614856905&amp;fm=173&amp;app=49&amp;f=JPEG?w=640&amp;h=197&amp;s=CF50C5108BF044014CD015D30000C0B0" alt="" /></a></blockquote>
来源： <em><a href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9310443071119729141%22%7D&amp;n_type=0&amp;p_from=1">从想法到实干，2018年13项NLP绝美新研究</a></em>
            </p>
        </div>
    </div>
</div>

<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="../static/js/bootstrap.min.js"></script>
</body>
</html>