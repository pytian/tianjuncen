<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title></title>
</head>
<body><!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="../static/css/bootstrap.min.css">

    <title>Daniel Tunkelang谈2019年深度学习在NLP上的展望</title>
    <style>
        ul {
            list-style-type: none;
        }

        li {
            float: left;
            margin: 20px;
        }

        .row {
            border: rgba(0,0,0,0.3) 1px solid;
            margin: 10px;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <p>
                Daniel Tunkelang谈2019年深度学习在NLP上的展望
            </p>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <p>
                <!-- wp:paragraph -->
<p></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>回顾2018年</strong>：<br></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>2018年，自然语言处理和理解的词嵌入的复杂性方面有了两大重要进展。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>第一次是在三月。艾伦人工智能研究所和华盛顿大学的研究人员发表了Deep contextualized word representations一文，提出了ELMo(Embeddings from Language Models)，这是一种开源的深度语境化词汇表示，改进了word2vec或GloVe这类上下文无关的嵌入。作者通过简单地替换ELMo预训练模型中的向量，证明了对现有NLP系统的改进。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>第二次是在11月。谷歌开源了BERT(Bidirectional Encoder Representations from Transformers)，这是一个双向的、无监督的语言表示，在维基百科语料上进行了预训练。正如作者在“BERT:用于语言理解的深层双向Transformers的预训练”一文中展示的，他们在各种NLP基准测试中取得了显著的改进，甚至比ELMo更强。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>从智能音箱的迅速普及(到2018年底将达到1亿台左右)到移动电话上数字助理的普及，自然语言理解的进步正迅速从实验室转移到现实世界。对于NLP研究和实践来说，这是一个激动人心的时代。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>展望2019年</strong>：</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>但我们还有很长的路要走。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>同样是在今年，艾伦研究所的研究人员发布了《Swag：用于基础常识推理的大型对抗式数据集》(Swag: A large - large Adversarial Dataset for Grounded Commonsense)，这是一个用于需要常识理解的句子完成任务的数据集。他们的实验表明，最先进的NLP仍然远远落后于人类的表现。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>但希望我们能在2019年看到更多的NLP突破。计算机科学领域许多最优秀的人才都在从事这方面的工作，工业界也渴望应用他们的成果。</p>
<!-- /wp:paragraph -->
            </p>
        </div>
    </div>
</div>

<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="../static/js/bootstrap.min.js"></script>
</body>
</html>